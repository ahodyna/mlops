from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, Response
from fastapi.middleware.cors import CORSMiddleware
import os
import tempfile
import shutil
from pathlib import Path
from typing import List, Optional
import json
from datetime import datetime
import asyncio
import uvicorn
import time
import cv2
import numpy as np
import pandas as pd

from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

# Evidently –¥–ª—è drift detection
try:
    from evidently.report import Report
    from evidently.metrics import DatasetDriftMetric
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False
    print("‚ö†Ô∏è Evidently –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. Drift detection –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π.")

from yolo_inference import YOLODogInference

app = FastAPI(
    title="üêï Dog Detection API",
    description="YOLOv8 Dog Detection Service with MLFlow and MinIO integration",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

predictions_total = Counter('dog_predictions_total', 'Total predictions made', ['status'])

# –ß–∞—Å –æ–±—Ä–æ–±–∫–∏
processing_time = Histogram(
    'dog_processing_seconds', 
    'Time spent processing images',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

dogs_detected = Counter('dogs_detected_total', 'Total dogs detected')

class SimpleDriftMonitor:
    def __init__(self):
        self.reference_data = []
        self.current_batch = []
        self.batch_size = 20
        
    def extract_features(self, image_data, processing_time, dogs_count):
        """–í–∏—Ç—è–≥—É—î –ø—Ä–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫–∏ –∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è"""
        nparr = np.frombuffer(image_data, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is not None:
            return {
                'width': img.shape[1],
                'height': img.shape[0],
                'brightness': float(np.mean(img)),
                'processing_time': processing_time,
                'dogs_count': dogs_count
            }
        return None
    
    def add_sample(self, image_data, processing_time, dogs_count):
        """–î–æ–¥–∞—î –∑—Ä–∞–∑–æ–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        features = self.extract_features(image_data, processing_time, dogs_count)
        if features:
            self.current_batch.append(features)
            
            if len(self.current_batch) >= self.batch_size:
                self.check_drift()
                self.current_batch = []
    
    def set_reference_data(self):
        if self.current_batch:
            self.reference_data = self.current_batch.copy()
            print(f"‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {len(self.reference_data)} –µ—Ç–∞–ª–æ–Ω–Ω–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤")
    
    def check_drift(self):   
        if not EVIDENTLY_AVAILABLE or not self.reference_data:
            return
            
        try:
            ref_df = pd.DataFrame(self.reference_data)
            curr_df = pd.DataFrame(self.current_batch)
            
            report = Report(metrics=[DatasetDriftMetric()])
            report.run(reference_data=ref_df, current_data=curr_df)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = f"drift_report_{timestamp}.html"
            report.save_html(report_path)
            
            results = report.as_dict()
            drift_detected = results['metrics'][0]['result']['dataset_drift']
            
            if drift_detected:
                print(f"üö® –í–ò–Ø–í–õ–ï–ù–û –î–†–Ü–§–¢! –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
            else:
                print(f"‚úÖ –î—Ä—ñ—Ñ—Ç –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ. –ó–≤—ñ—Ç: {report_path}")
                
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥—Ä—ñ—Ñ—Ç—É: {e}")

# Global services - –æ–≥–æ–ª–æ—à—É—î–º–æ —Ç—É—Ç, –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!
inference_service = None
drift_monitor = SimpleDriftMonitor()

@app.on_event("startup")
async def startup_event():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Ä–≤—ñ—Å—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫—É - test"""
    global inference_service  # ‚Üê –¢–µ–ø–µ—Ä global –ü–ï–†–ï–î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ü–µ CI/CD —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
    is_ci = os.getenv("CI", "false").lower() == "true"
    is_github_actions = os.getenv("GITHUB_ACTIONS", "false").lower() == "true"
    
    if is_ci or is_github_actions:
        print("üß™ CI/CD —Ä–µ–∂–∏–º - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—é MLflow")
        inference_service = None
        return
    
    try:
        mlflow_uri = os.getenv("MLFLOW_URI", "http://mlflow:5000")
        minio_endpoint = os.getenv("MINIO_ENDPOINT", "minio:9000")
        minio_access_key = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
        minio_secret_key = os.getenv("MINIO_SECRET_KEY", "minioadmin")
        minio_bucket = os.getenv("MINIO_BUCKET", "mlflow-artifacts")
        
        print(f"üîÑ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MLflow: {mlflow_uri}")
        print(f"üîÑ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MinIO: {minio_endpoint}")
        
        inference_service = YOLODogInference(
            mlflow_tracking_uri=mlflow_uri,
            minio_endpoint=minio_endpoint,
            minio_access_key=minio_access_key,
            minio_secret_key=minio_secret_key,
            minio_bucket=minio_bucket,
            confidence_threshold=0.5,
            iou_threshold=0.45
        )
        print("‚úÖ Dog Detection API –∑–∞–ø—É—â–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó: {e}")
        print("üí° API –ø—Ä–∞—Ü—é—î –≤ –æ–±–º–µ–∂–µ–Ω–æ–º—É —Ä–µ–∂–∏–º—ñ")
        inference_service = None

@app.get("/")
async def root():
    """–ì–æ–ª–æ–≤–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ API"""
    is_ci = os.getenv("CI", "false").lower() == "true"
    is_github_actions = os.getenv("GITHUB_ACTIONS", "false").lower() == "true"
    
    return {
        "message": "üêï Dog Detection API",
        "version": "1.0.0",
        "status": "running",
        "mode": "ci-test" if (is_ci or is_github_actions) else "production",
        "inference_available": inference_service is not None,
        "endpoints": {
            "health": "/health",
            "models": "/models",
            "predict": "/predict",
            "batch_predict": "/batch-predict",
            "metrics": "/metrics",
            "drift_baseline": "/set-drift-baseline"
        }
    }

@app.get("/health")
async def health_check():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤—ñ—Å—É"""
    is_ci = os.getenv("CI", "false").lower() == "true"
    is_github_actions = os.getenv("GITHUB_ACTIONS", "false").lower() == "true"
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "dog-detection-api",
        "mode": "ci-test" if (is_ci or is_github_actions) else "production",
        "inference_available": inference_service is not None
    }

@app.get("/metrics")
async def get_metrics():
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.post("/set-drift-baseline")
async def set_drift_baseline():
    """–í—Å—Ç–∞–Ω–æ–≤–ª—é—î –ø–æ—Ç–æ—á–Ω—ñ –¥–∞–Ω—ñ —è–∫ –µ—Ç–∞–ª–æ–Ω–Ω—ñ –¥–ª—è drift detection"""
    drift_monitor.set_reference_data()
    return {"message": "–ï—Ç–∞–ª–æ–Ω–Ω—ñ –¥–∞–Ω—ñ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ", "timestamp": datetime.now().isoformat()}

@app.get("/models")
async def list_models():
    """–û—Ç—Ä–∏–º–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    if not inference_service:
        raise HTTPException(status_code=503, detail="Inference —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (CI —Ä–µ–∂–∏–º –∞–±–æ –ø–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó)")
    
    try:
        models = inference_service.list_available_models()
        latest_model = inference_service.get_latest_model()
        
        return {
            "available_models": models,
            "latest_model": latest_model,
            "total_count": len(models)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π: {str(e)}")

@app.post("/predict")
async def predict_single_image(
    file: UploadFile = File(...),
    confidence: float = Query(0.5, ge=0.0, le=1.0, description="Confidence threshold"),
    iou: float = Query(0.45, ge=0.0, le=1.0, description="IoU threshold"),
    model_path: Optional[str] = Query(None, description="Specific model path")
):
    """–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –æ–¥–Ω–æ–º—É –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—ñ"""
    
    if not inference_service:
        raise HTTPException(status_code=503, detail="Inference —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (CI —Ä–µ–∂–∏–º –∞–±–æ –ø–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó)")
    
    start_time = time.time()
    
    if not file.content_type.startswith('image/'):
        predictions_total.labels(status='error').inc()
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º")
    
    temp_path = None
    try:
        image_data = await file.read()
        file.file.seek(0)
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{file.filename}") as temp_file:
            shutil.copyfileobj(file.file, temp_file)
            temp_path = temp_file.name
        
        inference_service.confidence_threshold = confidence
        inference_service.iou_threshold = iou
        
        model = inference_service.load_model(model_path)
        if model is None:
            predictions_total.labels(status='error').inc()
            raise HTTPException(status_code=500, detail="–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å")
        
        result = inference_service.run_inference_on_image(
            model, 
            Path(temp_path), 
            save_results=False
        )
        
        if result is None:
            predictions_total.labels(status='error').inc()
            raise HTTPException(status_code=500, detail="–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
        
        process_time = time.time() - start_time
        processing_time.observe(process_time)
        predictions_total.labels(status='success').inc()
        dogs_detected.inc(result['detection_count'])
        
        # –î–æ–¥–∞—î–º–æ –¥–æ drift monitoring
        drift_monitor.add_sample(image_data, process_time, result['detection_count'])
        
        return {
            "filename": file.filename,
            "detections": result['detections'],
            "detection_count": result['detection_count'],
            "processing_time": process_time,
            "confidence_threshold": confidence,
            "iou_threshold": iou,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        predictions_total.labels(status='error').inc()
        raise HTTPException(status_code=500, detail=f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É: {str(e)}")
    finally:
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except:
                pass

@app.post("/batch-predict")
async def batch_predict(
    files: List[UploadFile] = File(...),
    confidence: float = Query(0.5, ge=0.0, le=1.0),
    iou: float = Query(0.45, ge=0.0, le=1.0),
    model_path: Optional[str] = Query(None)
):
    """Batch —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –¥–µ–∫—ñ–ª—å–∫–æ—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö"""
    
    if not inference_service:
        raise HTTPException(status_code=503, detail="Inference —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (CI —Ä–µ–∂–∏–º –∞–±–æ –ø–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó)")
    
    if len(files) > 10:
        predictions_total.labels(status='error').inc()
        raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 10 —Ñ–∞–π–ª—ñ–≤ –∑–∞ —Ä–∞–∑")
    
    for file in files:
        if not file.content_type.startswith('image/'):
            predictions_total.labels(status='error').inc()
            raise HTTPException(status_code=400, detail=f"–§–∞–π–ª {file.filename} –Ω–µ —î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º")
    
    start_time = time.time()
    temp_files = []
    
    try:
        inference_service.confidence_threshold = confidence
        inference_service.iou_threshold = iou
        
        model = inference_service.load_model(model_path)
        if model is None:
            predictions_total.labels(status='error').inc()
            raise HTTPException(status_code=500, detail="–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å")
        
        results = []
        
        for file in files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{file.filename}") as temp_file:
                shutil.copyfileobj(file.file, temp_file)
                temp_path = temp_file.name
                temp_files.append(temp_path)
            
            result = inference_service.run_inference_on_image(
                model, 
                Path(temp_path), 
                save_results=False
            )
            
            if result:
                results.append({
                    "filename": file.filename,
                    "detections": result['detections'],
                    "detection_count": result['detection_count']
                })
                dogs_detected.inc(result['detection_count'])
        
        process_time = time.time() - start_time
        processing_time.observe(process_time)
        predictions_total.labels(status='success').inc()
        
        total_detections = sum(r['detection_count'] for r in results)
        images_with_dogs = len([r for r in results if r['detection_count'] > 0])
        
        return {
            "results": results,
            "summary": {
                "total_images": len(files),
                "processed_images": len(results),
                "total_detections": total_detections,
                "images_with_dogs": images_with_dogs,
                "processing_time": process_time,
                "confidence_threshold": confidence,
                "iou_threshold": iou,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        predictions_total.labels(status='error').inc()
        raise HTTPException(status_code=500, detail=f"–ü–æ–º–∏–ª–∫–∞ batch —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É: {str(e)}")
    finally:
        for temp_path in temp_files:
            try:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
            except:
                pass

@app.get("/stats")
async def get_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Ä–≤—ñ—Å—É"""
    return {
        "service": "dog-detection-api",
        "model_info": {
            "latest_model": inference_service.get_latest_model() if inference_service else None,
            "confidence_threshold": inference_service.confidence_threshold if inference_service else None,
            "iou_threshold": inference_service.iou_threshold if inference_service else None,
            "available": inference_service is not None
        },
        "system_info": {
            "python_version": os.sys.version,
            "evidently_available": EVIDENTLY_AVAILABLE,
            "timestamp": datetime.now().isoformat()
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )